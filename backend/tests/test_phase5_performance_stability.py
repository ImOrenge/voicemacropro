"""
Phase 5: ì„±ëŠ¥ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™” í…ŒìŠ¤íŠ¸
VoiceMacro Proì˜ TDD ê¸°ë°˜ ê°œë°œì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ
"""

import os
import sys
import json
import pytest
import asyncio
import time
import threading
import gc
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass
import weakref

# psutilì„ ì•ˆì „í•˜ê²Œ import
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    # psutilì´ ì—†ì„ ê²½ìš° ëª¨ì˜ êµ¬í˜„
    class MockProcess:
        def memory_info(self):
            return type('MemoryInfo', (), {'rss': 100 * 1024 * 1024})()  # 100MB
    
    class MockPsutil:
        @staticmethod
        def cpu_percent(interval=None):
            return 10.0  # 10% CPU ì‚¬ìš©ë¥ 
        
        @staticmethod
        def Process():
            return MockProcess()
    
    psutil = MockPsutil()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    cpu_usage: float
    memory_usage: float  # MB
    response_time: float  # ms
    throughput: float  # requests/sec
    error_rate: float  # %
    timestamp: float


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™”"""
        self.metrics_history: List[PerformanceMetrics] = []
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.monitor_interval = 0.1  # 100ms ê°„ê²©
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            return
            
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_monitoring:
            try:
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
                cpu_percent = psutil.cpu_percent(interval=None)
                memory_info = psutil.Process().memory_info()
                memory_mb = memory_info.rss / 1024 / 1024  # bytes to MB
                
                # ë©”íŠ¸ë¦­ ìƒì„±
                metrics = PerformanceMetrics(
                    cpu_usage=cpu_percent,
                    memory_usage=memory_mb,
                    response_time=0.0,  # ê°œë³„ ì¸¡ì • í•„ìš”
                    throughput=0.0,     # ê°œë³„ ì¸¡ì • í•„ìš”
                    error_rate=0.0,     # ê°œë³„ ì¸¡ì • í•„ìš”
                    timestamp=time.time()
                )
                
                self.metrics_history.append(metrics)
                
                # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œ)
                if len(self.metrics_history) > 1000:
                    self.metrics_history.pop(0)
                
                time.sleep(self.monitor_interval)
                
            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                break
    
    def get_average_metrics(self, last_n: int = 10) -> Optional[PerformanceMetrics]:
        """ìµœê·¼ Nê°œ ë©”íŠ¸ë¦­ì˜ í‰ê·  ë°˜í™˜"""
        if len(self.metrics_history) < last_n:
            return None
            
        recent_metrics = self.metrics_history[-last_n:]
        
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        avg_response = sum(m.response_time for m in recent_metrics) / len(recent_metrics)
        avg_throughput = sum(m.throughput for m in recent_metrics) / len(recent_metrics)
        avg_error_rate = sum(m.error_rate for m in recent_metrics) / len(recent_metrics)
        
        return PerformanceMetrics(
            cpu_usage=avg_cpu,
            memory_usage=avg_memory,
            response_time=avg_response,
            throughput=avg_throughput,
            error_rate=avg_error_rate,
            timestamp=time.time()
        )


class MemoryLeakDetector:
    """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ê¸° ì´ˆê¸°í™”"""
        self.initial_objects = None
        self.object_refs = []
        
    def start_detection(self):
        """ëˆ„ìˆ˜ ê°ì§€ ì‹œì‘"""
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        self.initial_objects = len(gc.get_objects())
        self.object_refs.clear()
    
    def check_for_leaks(self, threshold: int = 1000) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸"""
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        current_objects = len(gc.get_objects())
        
        leak_info = {
            "initial_objects": self.initial_objects,
            "current_objects": current_objects,
            "object_increase": current_objects - self.initial_objects if self.initial_objects else 0,
            "potential_leak": False,
            "memory_usage": psutil.Process().memory_info().rss / 1024 / 1024  # MB
        }
        
        if leak_info["object_increase"] > threshold:
            leak_info["potential_leak"] = True
            
        return leak_info
    
    def track_object(self, obj: Any, description: str = ""):
        """ê°ì²´ ì¶”ì  ë“±ë¡"""
        try:
            weak_ref = weakref.ref(obj)
            self.object_refs.append({
                "ref": weak_ref,
                "description": description,
                "created_at": time.time()
            })
        except TypeError:
            # ì¼ë¶€ ê°ì²´ëŠ” weakrefë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            pass
    
    def check_tracked_objects(self) -> Dict[str, Any]:
        """ì¶”ì ëœ ê°ì²´ ìƒíƒœ í™•ì¸"""
        alive_count = 0
        dead_count = 0
        
        for ref_info in self.object_refs:
            if ref_info["ref"]() is not None:
                alive_count += 1
            else:
                dead_count += 1
        
        return {
            "total_tracked": len(self.object_refs),
            "alive_objects": alive_count,
            "dead_objects": dead_count,
            "cleanup_rate": dead_count / len(self.object_refs) if self.object_refs else 0
        }


class StressTestRunner:
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì´ˆê¸°í™”"""
        self.test_results = []
        self.error_count = 0
        self.success_count = 0
        
    async def run_concurrent_load_test(self, target_function: Callable, 
                                      concurrent_count: int = 10, 
                                      duration_seconds: int = 5) -> Dict[str, Any]:
        """ë™ì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.perf_counter()
        end_time = start_time + duration_seconds
        
        tasks = []
        results = []
        
        async def worker():
            """ì›Œì»¤ í•¨ìˆ˜"""
            worker_results = []
            while time.perf_counter() < end_time:
                try:
                    task_start = time.perf_counter()
                    result = await target_function()
                    task_end = time.perf_counter()
                    
                    worker_results.append({
                        "success": True,
                        "response_time": (task_end - task_start) * 1000,  # ms
                        "result": result
                    })
                    self.success_count += 1
                    
                except Exception as e:
                    worker_results.append({
                        "success": False,
                        "error": str(e),
                        "response_time": 0
                    })
                    self.error_count += 1
                
                # ì‘ì€ ë”œë ˆì´ë¡œ CPU ì‚¬ìš©ë¥  ì¡°ì ˆ
                await asyncio.sleep(0.001)
            
            return worker_results
        
        # ë™ì‹œ ì›Œì»¤ ì‹¤í–‰
        for _ in range(concurrent_count):
            task = asyncio.create_task(worker())
            tasks.append(task)
        
        # ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
        worker_results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ì§‘ê³„
        all_results = []
        for worker_result in worker_results:
            all_results.extend(worker_result)
        
        # í†µê³„ ê³„ì‚°
        successful_results = [r for r in all_results if r["success"]]
        failed_results = [r for r in all_results if not r["success"]]
        
        if successful_results:
            response_times = [r["response_time"] for r in successful_results]
            avg_response_time = sum(response_times) / len(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = max_response_time = min_response_time = 0
        
        total_duration = time.perf_counter() - start_time
        throughput = len(all_results) / total_duration if total_duration > 0 else 0
        error_rate = len(failed_results) / len(all_results) * 100 if all_results else 0
        
        return {
            "total_requests": len(all_results),
            "successful_requests": len(successful_results),
            "failed_requests": len(failed_results),
            "error_rate": error_rate,
            "avg_response_time": avg_response_time,
            "max_response_time": max_response_time,
            "min_response_time": min_response_time,
            "throughput": throughput,
            "duration": total_duration,
            "concurrent_workers": concurrent_count
        }


class TestPerformanceMonitoring:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def performance_monitor(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„° í”½ìŠ¤ì²˜"""
        monitor = PerformanceMonitor()
        yield monitor
        monitor.stop_monitoring()
    
    def test_monitor_initialization(self, performance_monitor):
        """âœ… Test 1: ëª¨ë‹ˆí„° ì´ˆê¸°í™”"""
        assert not performance_monitor.is_monitoring
        assert len(performance_monitor.metrics_history) == 0
        assert performance_monitor.monitor_interval == 0.1
    
    def test_monitoring_start_stop(self, performance_monitor):
        """âœ… Test 2: ëª¨ë‹ˆí„°ë§ ì‹œì‘/ì¤‘ì§€"""
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        performance_monitor.start_monitoring()
        assert performance_monitor.is_monitoring is True
        
        # ì ì‹œ ëŒ€ê¸° í›„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
        time.sleep(0.3)
        assert len(performance_monitor.metrics_history) > 0
        
        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        performance_monitor.stop_monitoring()
        assert performance_monitor.is_monitoring is False
    
    def test_metrics_collection(self, performance_monitor):
        """âœ… Test 3: ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        performance_monitor.start_monitoring()
        time.sleep(0.5)  # 500ms ë™ì•ˆ ìˆ˜ì§‘
        performance_monitor.stop_monitoring()
        
        # ë©”íŠ¸ë¦­ì´ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert len(performance_monitor.metrics_history) >= 3  # ìµœì†Œ 3ê°œ ìˆ˜ì§‘
        
        # ë©”íŠ¸ë¦­ ë‚´ìš© ê²€ì¦
        for metric in performance_monitor.metrics_history:
            assert metric.cpu_usage >= 0
            assert metric.memory_usage > 0  # ë©”ëª¨ë¦¬ëŠ” í•­ìƒ ì‚¬ìš© ì¤‘
            assert metric.timestamp > 0
    
    def test_average_metrics_calculation(self, performance_monitor):
        """âœ… Test 4: í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        performance_monitor.start_monitoring()
        time.sleep(0.5)
        performance_monitor.stop_monitoring()
        
        avg_metrics = performance_monitor.get_average_metrics(last_n=3)
        
        if avg_metrics:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            assert avg_metrics.cpu_usage >= 0
            assert avg_metrics.memory_usage > 0
        else:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°
            assert len(performance_monitor.metrics_history) < 3


class TestMemoryManagement:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def memory_detector(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ê¸° í”½ìŠ¤ì²˜"""
        return MemoryLeakDetector()
    
    def test_memory_leak_detection_initialization(self, memory_detector):
        """âœ… Test 5: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ ì´ˆê¸°í™”"""
        assert memory_detector.initial_objects is None
        assert len(memory_detector.object_refs) == 0
    
    def test_memory_baseline_establishment(self, memory_detector):
        """âœ… Test 6: ë©”ëª¨ë¦¬ ê¸°ì¤€ì„  ì„¤ì •"""
        memory_detector.start_detection()
        
        assert memory_detector.initial_objects is not None
        assert memory_detector.initial_objects > 0
    
    def test_object_tracking(self, memory_detector):
        """âœ… Test 7: ê°ì²´ ì¶”ì """
        memory_detector.start_detection()
        
        # í…ŒìŠ¤íŠ¸ ê°ì²´ ìƒì„± ë° ì¶”ì 
        test_objects = []
        for i in range(10):
            obj = {"id": i, "data": f"test_data_{i}"}
            test_objects.append(obj)
            memory_detector.track_object(obj, f"test_object_{i}")
        
        # ì¶”ì  ìƒíƒœ í™•ì¸
        status = memory_detector.check_tracked_objects()
        assert status["total_tracked"] == 10
        assert status["alive_objects"] == 10
        assert status["dead_objects"] == 0
        
        # ê°ì²´ í•´ì œ
        del test_objects
        gc.collect()
        
        # ì •ë¦¬ í™•ì¸
        status_after = memory_detector.check_tracked_objects()
        assert status_after["dead_objects"] > 0
    
    def test_memory_leak_simulation(self, memory_detector):
        """âš ï¸ Test 8: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì‹œë®¬ë ˆì´ì…˜"""
        memory_detector.start_detection()
        
        # ì˜ë„ì ì¸ ë©”ëª¨ë¦¬ "ëˆ„ìˆ˜" ì‹œë®¬ë ˆì´ì…˜
        leaked_objects = []
        for i in range(100):  # ì ì€ ìˆ˜ë¡œ ì‹œë®¬ë ˆì´ì…˜
            obj = [f"data_{j}" for j in range(100)]  # í° ê°ì²´
            leaked_objects.append(obj)
        
        # ëˆ„ìˆ˜ í™•ì¸
        leak_info = memory_detector.check_for_leaks(threshold=50)
        
        # ëˆ„ìˆ˜ê°€ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë˜ëŠ” í—ˆìš© ê°€ëŠ¥í•œ ì¦ê°€ì¸ì§€)
        assert leak_info["object_increase"] > 0
        assert leak_info["memory_usage"] > 0
        
        # ì •ë¦¬
        del leaked_objects
        gc.collect()


class TestStressAndLoad:
    """ìŠ¤íŠ¸ë ˆìŠ¤ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def stress_runner(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° í”½ìŠ¤ì²˜"""
        return StressTestRunner()
    
    async def mock_voice_recognition_task(self):
        """ëª¨ì˜ ìŒì„± ì¸ì‹ ì‘ì—…"""
        # ì‹¤ì œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        await asyncio.sleep(0.01)  # 10ms ì²˜ë¦¬ ì‹œê°„
        
        # ëœë¤ ê²°ê³¼ ìƒì„±
        import random
        if random.random() < 0.95:  # 95% ì„±ê³µë¥ 
            return {
                "text": "ê³µê²©í•´",
                "confidence": 0.9 + random.random() * 0.1
            }
        else:
            raise Exception("Recognition failed")
    
    @pytest.mark.asyncio
    async def test_concurrent_voice_processing(self, stress_runner):
        """âš¡ Test 9: ë™ì‹œ ìŒì„± ì²˜ë¦¬ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        result = await stress_runner.run_concurrent_load_test(
            target_function=self.mock_voice_recognition_task,
            concurrent_count=10,
            duration_seconds=2
        )
        
        # ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
        assert result["error_rate"] < 10, f"ì˜¤ë¥˜ìœ¨ {result['error_rate']:.1f}% > 10%"
        assert result["avg_response_time"] < 50, f"í‰ê·  ì‘ë‹µì‹œê°„ {result['avg_response_time']:.1f}ms > 50ms"
        assert result["throughput"] > 50, f"ì²˜ë¦¬ëŸ‰ {result['throughput']:.1f} req/s < 50 req/s"
        
        # ê¸°ë³¸ í†µê³„ í™•ì¸
        assert result["total_requests"] > 0
        assert result["successful_requests"] > 0
    
    async def mock_macro_execution_task(self):
        """ëª¨ì˜ ë§¤í¬ë¡œ ì‹¤í–‰ ì‘ì—…"""
        # ë‹¤ì–‘í•œ ë§¤í¬ë¡œ íƒ€ì… ì‹œë®¬ë ˆì´ì…˜
        import random
        macro_types = ["key", "combo", "rapid", "hold"]
        macro_type = random.choice(macro_types)
        
        if macro_type == "key":
            await asyncio.sleep(0.001)  # 1ms
        elif macro_type == "combo":
            await asyncio.sleep(0.01)   # 10ms
        elif macro_type == "rapid":
            await asyncio.sleep(0.05)   # 50ms
        elif macro_type == "hold":
            await asyncio.sleep(0.02)   # 20ms
        
        return {"type": macro_type, "success": True}
    
    @pytest.mark.asyncio
    async def test_high_frequency_macro_execution(self, stress_runner):
        """âš¡ Test 10: ê³ ë¹ˆë„ ë§¤í¬ë¡œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        result = await stress_runner.run_concurrent_load_test(
            target_function=self.mock_macro_execution_task,
            concurrent_count=5,
            duration_seconds=3
        )
        
        # ë§¤í¬ë¡œ ì‹¤í–‰ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
        assert result["error_rate"] < 5, f"ë§¤í¬ë¡œ ì‹¤í–‰ ì˜¤ë¥˜ìœ¨ {result['error_rate']:.1f}% > 5%"
        assert result["avg_response_time"] < 30, f"í‰ê·  ì‹¤í–‰ì‹œê°„ {result['avg_response_time']:.1f}ms > 30ms"
        
        # ì²˜ë¦¬ëŸ‰ í™•ì¸
        assert result["total_requests"] > 100  # 3ì´ˆ ë™ì•ˆ ìµœì†Œ 100ê°œ ì²˜ë¦¬
    
    @pytest.mark.asyncio
    async def test_system_resource_usage(self):
        """ğŸ”’ Test 11: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ë¶€í•˜ ìƒì„±
        stress_runner = StressTestRunner()
        await stress_runner.run_concurrent_load_test(
            target_function=self.mock_voice_recognition_task,
            concurrent_count=8,
            duration_seconds=2
        )
        
        monitor.stop_monitoring()
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
        avg_metrics = monitor.get_average_metrics(last_n=10)
        
        if avg_metrics:
            # ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
            assert avg_metrics.cpu_usage < 80, f"CPU ì‚¬ìš©ë¥  {avg_metrics.cpu_usage:.1f}% > 80%"
            assert avg_metrics.memory_usage < 500, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ {avg_metrics.memory_usage:.1f}MB > 500MB"
    
    @pytest.mark.asyncio
    async def test_sustained_load_stability(self, stress_runner):
        """ğŸ”„ Test 12: ì§€ì†ì  ë¶€í•˜ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        # ë” ê¸´ ì‹œê°„ ë™ì•ˆ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        result = await stress_runner.run_concurrent_load_test(
            target_function=self.mock_voice_recognition_task,
            concurrent_count=3,
            duration_seconds=5  # 5ì´ˆê°„ ì§€ì†
        )
        
        # ì•ˆì •ì„± ìš”êµ¬ì‚¬í•­
        assert result["error_rate"] < 15, f"ì§€ì† ë¶€í•˜ ì˜¤ë¥˜ìœ¨ {result['error_rate']:.1f}% > 15%"
        
        # ì„±ëŠ¥ ì €í•˜ ì—†ì´ ì§€ì†ì  ì²˜ë¦¬ í™•ì¸
        assert result["total_requests"] > 200  # 5ì´ˆ ë™ì•ˆ ìµœì†Œ 200ê°œ ì²˜ë¦¬
        assert result["throughput"] > 30  # ìµœì†Œ 30 req/s ìœ ì§€


class TestErrorRecovery:
    """ì˜¤ë¥˜ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
    
    async def faulty_service_task(self, failure_rate: float = 0.3):
        """ì˜ë„ì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ëŠ” ì„œë¹„ìŠ¤ ì‘ì—…"""
        import random
        await asyncio.sleep(0.005)  # 5ms ì²˜ë¦¬ ì‹œê°„
        
        if random.random() < failure_rate:
            raise Exception("Service temporarily unavailable")
        
        return {"status": "success", "data": "processed"}
    
    @pytest.mark.asyncio
    async def test_error_handling_resilience(self):
        """ğŸ›¡ï¸ Test 13: ì˜¤ë¥˜ ì²˜ë¦¬ ë³µì›ë ¥"""
        error_count = 0
        success_count = 0
        
        # ì—¬ëŸ¬ ë²ˆ ì‹œë„í•˜ì—¬ ì˜¤ë¥˜ ë³µêµ¬ í…ŒìŠ¤íŠ¸
        for _ in range(100):
            try:
                result = await self.faulty_service_task(failure_rate=0.2)
                success_count += 1
            except Exception:
                error_count += 1
        
        # ë³µì›ë ¥ ê²€ì¦
        total_attempts = success_count + error_count
        error_rate = error_count / total_attempts * 100
        
        assert total_attempts == 100
        assert success_count > 70  # 70% ì´ìƒ ì„±ê³µ
        assert error_rate < 30     # 30% ë¯¸ë§Œ ì‹¤íŒ¨
    
    @pytest.mark.asyncio
    async def test_graceful_degradation(self):
        """ğŸ“‰ Test 14: ì ì§„ì  ì„±ëŠ¥ ì €í•˜"""
        response_times = []
        
        # ì ì§„ì ìœ¼ë¡œ ë¶€í•˜ ì¦ê°€
        for load_level in [1, 3, 5, 8, 10]:
            start_time = time.perf_counter()
            
            tasks = []
            for _ in range(load_level):
                task = asyncio.create_task(self.mock_voice_recognition_task())
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.perf_counter()
            response_time = (end_time - start_time) * 1000  # ms
            response_times.append(response_time)
        
        # ì„±ëŠ¥ ì €í•˜ê°€ ì ì§„ì ì¸ì§€ í™•ì¸ (ê¸‰ê²©í•œ ì¦ê°€ ë°©ì§€)
        for i in range(1, len(response_times)):
            prev_time = response_times[i-1]
            curr_time = response_times[i]
            
            # ì´ì „ ëŒ€ë¹„ 10ë°° ì´ìƒ ì¦ê°€í•˜ì§€ ì•Šì•„ì•¼ í•¨
            assert curr_time < prev_time * 10, f"ê¸‰ê²©í•œ ì„±ëŠ¥ ì €í•˜: {prev_time:.1f}ms â†’ {curr_time:.1f}ms"
    
    async def mock_voice_recognition_task(self):
        """ëª¨ì˜ ìŒì„± ì¸ì‹ ì‘ì—… (ê°„ë‹¨ ë²„ì „)"""
        await asyncio.sleep(0.01)
        return {"text": "recognized", "confidence": 0.9}


class TestDataIntegrity:
    """ë°ì´í„° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸"""
    
    def test_concurrent_data_access(self):
        """ğŸ”’ Test 15: ë™ì‹œ ë°ì´í„° ì ‘ê·¼"""
        import threading
        
        # ê³µìœ  ë°ì´í„° êµ¬ì¡°
        shared_data = {"counter": 0, "items": []}
        lock = threading.Lock()
        
        def worker(worker_id: int):
            """ì›Œì»¤ í•¨ìˆ˜"""
            for i in range(100):
                with lock:
                    shared_data["counter"] += 1
                    shared_data["items"].append(f"worker_{worker_id}_item_{i}")
        
        # ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œ ì ‘ê·¼
        threads = []
        for worker_id in range(5):
            thread = threading.Thread(target=worker, args=(worker_id,))
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()
        
        # ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
        assert shared_data["counter"] == 500  # 5 workers * 100 items
        assert len(shared_data["items"]) == 500
        
        # ì¤‘ë³µ ì—†ëŠ”ì§€ í™•ì¸
        unique_items = set(shared_data["items"])
        assert len(unique_items) == 500
    
    def test_configuration_consistency(self):
        """âš™ï¸ Test 16: ì„¤ì • ì¼ê´€ì„±"""
        # ëª¨ì˜ ì„¤ì • ë°ì´í„°
        config = {
            "audio": {
                "sample_rate": 24000,
                "channels": 1,
                "buffer_size": 2400
            },
            "voice_recognition": {
                "confidence_threshold": 0.7,
                "model": "gpt-4o-transcribe"
            },
            "performance": {
                "max_cpu_usage": 15,
                "max_memory_usage": 200,
                "max_response_time": 300
            }
        }
        
        # ì„¤ì • ê²€ì¦ í•¨ìˆ˜
        def validate_config(cfg: Dict[str, Any]) -> List[str]:
            errors = []
            
            # ì˜¤ë””ì˜¤ ì„¤ì • ê²€ì¦
            if cfg["audio"]["sample_rate"] not in [16000, 24000, 48000]:
                errors.append("Invalid sample rate")
            
            if cfg["audio"]["channels"] not in [1, 2]:
                errors.append("Invalid channel count")
            
            # ì„±ëŠ¥ ì„¤ì • ê²€ì¦
            if cfg["performance"]["max_cpu_usage"] > 50:
                errors.append("CPU usage limit too high")
            
            if cfg["performance"]["max_memory_usage"] > 1000:
                errors.append("Memory usage limit too high")
            
            return errors
        
        # ì„¤ì • ê²€ì¦
        validation_errors = validate_config(config)
        assert len(validation_errors) == 0, f"ì„¤ì • ì˜¤ë¥˜: {validation_errors}"
        
        # ì¼ê´€ì„± í™•ì¸
        buffer_size = config["audio"]["buffer_size"]
        sample_rate = config["audio"]["sample_rate"]
        channels = config["audio"]["channels"]
        
        # ë²„í¼ í¬ê¸°ê°€ 100msì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸
        samples_per_100ms = (sample_rate * channels * 100) // 1000
        assert buffer_size == samples_per_100ms, "ë²„í¼ í¬ê¸°ê°€ 100msì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ"


class TestSystemIntegration:
    """ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.mark.asyncio
    async def test_end_to_end_performance(self):
        """ğŸ”„ Test 17: ì¢…ë‹¨ê°„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
        async def full_pipeline():
            """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
            # 1. ìŒì„± ìº¡ì²˜ (ì‹œë®¬ë ˆì´ì…˜)
            await asyncio.sleep(0.005)
            
            # 2. ìŒì„± ì¸ì‹
            await asyncio.sleep(0.02)
            
            # 3. ë§¤í¬ë¡œ ë§¤ì¹­
            await asyncio.sleep(0.003)
            
            # 4. ë§¤í¬ë¡œ ì‹¤í–‰
            await asyncio.sleep(0.01)
            
            return {"pipeline": "completed", "total_time": 0.038}
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.perf_counter()
        
        # ì—¬ëŸ¬ íŒŒì´í”„ë¼ì¸ ë™ì‹œ ì‹¤í–‰
        tasks = [full_pipeline() for _ in range(20)]
        results = await asyncio.gather(*tasks)
        
        end_time = time.perf_counter()
        total_time = (end_time - start_time) * 1000  # ms
        
        # ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
        assert len(results) == 20
        assert total_time < 1000, f"20ê°œ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œê°„ {total_time:.1f}ms > 1000ms"
        
        # í‰ê·  íŒŒì´í”„ë¼ì¸ ì‹œê°„
        avg_pipeline_time = total_time / 20
        assert avg_pipeline_time < 300, f"í‰ê·  íŒŒì´í”„ë¼ì¸ ì‹œê°„ {avg_pipeline_time:.1f}ms > 300ms"
    
    @pytest.mark.asyncio
    async def test_system_stability_under_load(self):
        """ğŸ›¡ï¸ Test 18: ë¶€í•˜ ìƒí™©ì—ì„œ ì‹œìŠ¤í…œ ì•ˆì •ì„±"""
        monitor = PerformanceMonitor()
        leak_detector = MemoryLeakDetector()
        
        monitor.start_monitoring()
        leak_detector.start_detection()
        
        # ì§€ì†ì ì¸ ë¶€í•˜ ìƒì„±
        async def sustained_load():
            for _ in range(50):
                await asyncio.sleep(0.01)  # 10ms ê°„ê²©ìœ¼ë¡œ ì‘ì—…
                
                # ëª¨ì˜ ì‘ì—… ì‹¤í–‰
                await self.mock_voice_recognition_task()
        
        # ì—¬ëŸ¬ ë¶€í•˜ ìƒì„±ê¸° ë™ì‹œ ì‹¤í–‰
        load_tasks = [sustained_load() for _ in range(3)]
        await asyncio.gather(*load_tasks)
        
        monitor.stop_monitoring()
        
        # ì•ˆì •ì„± ê²€ì¦
        avg_metrics = monitor.get_average_metrics(last_n=10)
        leak_info = leak_detector.check_for_leaks(threshold=500)
        
        if avg_metrics:
            assert avg_metrics.cpu_usage < 90, "ë†’ì€ ë¶€í•˜ì—ì„œ CPU ì‚¬ìš©ë¥  ê³¼ë‹¤"
            assert avg_metrics.memory_usage < 600, "ë†’ì€ ë¶€í•˜ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³¼ë‹¤"
        
        assert not leak_info["potential_leak"], "ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì¤‘ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€"
    
    async def mock_voice_recognition_task(self):
        """ëª¨ì˜ ìŒì„± ì¸ì‹ ì‘ì—…"""
        await asyncio.sleep(0.01)
        return {"status": "completed"}


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def run_phase5_tests():
    """Phase 5 í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    import subprocess
    
    print("ğŸ§ª Phase 5: ì„±ëŠ¥ ìµœì í™” ë° ì•ˆì •ì„± ê°•í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    # pytest ì‹¤í–‰
    result = subprocess.run([
        'py', '-m', 'pytest', 
        __file__,
        '-v',  # ìƒì„¸ ì¶œë ¥
        '--tb=short',  # ì§§ì€ íŠ¸ë ˆì´ìŠ¤ë°±
        '--disable-warnings'  # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¹€
    ], capture_output=True, text=True)
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(result.stdout)
    
    if result.stderr:
        print("âš ï¸ ê²½ê³ /ì˜¤ë¥˜:")
        print(result.stderr)
    
    return result.returncode == 0


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    success = run_phase5_tests()
    
    if success:
        print("âœ… Phase 5 í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼!")
        print("ğŸ‰ ëª¨ë“  Phase êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. êµ¬í˜„ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.") 